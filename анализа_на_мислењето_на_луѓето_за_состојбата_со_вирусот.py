# -*- coding: utf-8 -*-
"""Анализа на мислењето на луѓето за состојбата со вирусот.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/132pi9hmlE6nZlRKchCKLHbGyB2jcOMlb

<img align="left" style="padding-right:10px;" src="https://www.finki.ukim.mk/sites/default/files/styles/medium/public/default_images/finki_52_1_2_1_62_0.png?itok=636vJHVG">

*Семинарска работа по предметот Вовед во науката за податоците*

# Анализа на мислењето на луѓето за состојбата со вирусот


---

Изработиле: 

*   Тамара Даниловска 183079
*   Миле Јанкулоски 183035

Ментори: 

*   проф. д-р Димитар Трајанов
*   асс. Јована Добрева

# Апстракт

> Поради ужасната состојба во која се наоѓаме, сакавме да изработиме проект каде што ќе го видиме мислењето на луѓето во светот за состојбата со Корона Вирусот. Направивме анализа за моменталната состојба со вирусот но таа не ни помогна многу. Затоа одлучивме да искористиме нешто што на нас ни е многу блиску и тоа беа социјалните мрежи.

> Најдовме податоци со твитови од различни луѓе од различни држави и истиот одлучивме да го искористиме за сегментална анализа. За тренирање на моделот искористивме мислење на илјадници твитови во врска со состојбата за вирусот, додека пак предвидувањето го направивме со твитови за моменталната состојба со вакцинацијата на населението. 

> Од анализата се заклучи дека бројот на случаи се намалува секојдневно и мислењето на луѓето за вирусот останува неутрално.

# Вовед

## Иницијализација на библиотеки
"""

import re
import math
import random
from collections import Counter, defaultdict
import pandas as pd
import numpy as np
import seaborn as sns
import csv
import nltk
from nltk.corpus import stopwords, movie_reviews
from datetime import timedelta, date
import requests
from pandas import json_normalize
import json

nltk.download('stopwords')

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from PIL import Image

from keras.preprocessing.text import Tokenizer

from keras.preprocessing import sequence

import keras.backend as K
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM, Flatten
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

"""## Прибирање и екстракција на податоци

Поврзување со google mount
"""

from google.colab import drive
drive.mount('/content/drive')

"""Ова прибирање се прави за последните x дена. Максималната вредност што го дозволува ова api за денови е 21. Визуализацијата е направена за последните 2 недели."""

# max = 21
x = input()

date_to = date.today() - timedelta(days = 1)
date_from = date_to - timedelta(days = int(x))
datesPerCountry = []

url = 'https://api.covid19tracking.narrativa.com/api?date_from='+date_from.strftime("%Y-%m-%d")+'&date_to='+date_to.strftime("%Y-%m-%d")
response = requests.get(url)

strResp = json.dumps(response.json())
obj = json.loads(strResp)

dictDates = obj["dates"]
dates = list(dictDates.keys())

dictCountries = dictDates[date_to.strftime("%Y-%m-%d")]["countries"]
countries = list(dictCountries.keys())

"""Поради тоа што API-то повеќе не содржи податоци за државата Кирибати, рачно ја бришеме од листата на држави."""

countries.remove('Kiribati')

"""### Прибирање на податоци за вкупниот број на случаи по држава

---



"""

clmns = ['today_confirmed', 'today_deaths', 'today_new_confirmed', 'today_new_deaths', 'today_new_open_cases', 'today_new_recovered', 'today_open_cases', 'today_recovered']

all_countries = []
for country in countries:
  per_country_stats = []
  for date in dates:
    per_country_stats.append([dictDates[date]["countries"][country]['today_confirmed'],
                    dictDates[date]["countries"][country]['today_deaths'],
                    dictDates[date]["countries"][country]['today_new_confirmed'],
                    dictDates[date]["countries"][country]['today_new_deaths'],
                    dictDates[date]["countries"][country]['today_new_open_cases'],
                    dictDates[date]["countries"][country]['today_new_recovered'],
                    dictDates[date]["countries"][country]['today_open_cases'],
                    dictDates[date]["countries"][country]['today_recovered']])
  df = pd.DataFrame(per_country_stats, index=dates, columns=clmns)
  all_countries.append([country, df])

all_countries[0][1].head()

all_countries[0][1].shape

"""### Прибирање на податоци за вкупниот број на случаи по датуми

"""

urls_to_get = []
for date in dates:
  urls_to_get.append('https://api.covid19tracking.narrativa.com/api/' + date)
urls_to_get

total = []
for i in range(14):
  resp = requests.get(urls_to_get[i])
  totalStrResp = json.dumps(resp.json())
  totalObj = json.loads(totalStrResp)
  print(urls_to_get[i])
  total.append([dates[i], totalObj["total"]["today_confirmed"],
  totalObj["total"]['today_deaths'],
  totalObj["total"]['today_new_confirmed'],
  totalObj["total"]['today_new_deaths'],
  totalObj["total"]['today_new_open_cases'],
  totalObj["total"]['today_new_recovered'],
  totalObj["total"]['today_open_cases'],
  totalObj["total"]['today_recovered']])

newClmns = ['date', *clmns]
totalDf = pd.DataFrame(total, columns=newClmns)
totalDf.head()

"""## Читање на податоците"""

data_tweets_train = pd.read_csv('/content/drive/MyDrive/Corona_NLP_test.csv')
data_tweets_test = pd.read_csv("/content/drive/MyDrive/vaccination_tweets.csv")

data_tweets_test.head()

data_tweets_train.head()

data_tweets_test.shape

data_tweets_train.shape

"""##Чистење на податоците"""

tweets_train = data_tweets_train[["OriginalTweet"]]
tweets_test = data_tweets_test[["text"]]

"""### Мали букви"""

tweets_train["text_lower"] = tweets_train["OriginalTweet"].str.lower()
tweets_test["text_lower"] = tweets_test["text"].str.lower()

"""###Remоve stopwords"""

stopwords = set(stopwords.words("english"))

def remove_stopwords(text):
    return " ".join([words for words in str(text).split() if words not in stopwords])

tweets_train["text_without_stopwords"] = tweets_train["text_lower"].apply(lambda text: remove_stopwords(text))    
tweets_test["text_without_stopwords"] = tweets_test["text_lower"].apply(lambda text: remove_stopwords(text))

"""### Remove punctuation"""

punctuations =  '''!()-[]{};:"\,<>./?@#$%^&*_~'''

def remove_punctuations(text):
    return text.translate(str.maketrans("", "", punctuations))

tweets_train["text_without_punctuations"] = tweets_train["text_without_stopwords"].apply(lambda text: remove_punctuations(text))
tweets_test["text_without_punctuations"] = tweets_test["text_without_stopwords"].apply(lambda text: remove_punctuations(text))

"""## Одредување на класата"""

labels = data_tweets_train["Sentiment"]
tweets_train["labels"] = labels

def reassign_label(x):
    if x == "Extremely Positive" or x == "Positive":
        return 1
    elif x =="Extremely Negative" or x =="Negative":
        return -1
    elif x =="Neutral":
        return 0

tweets_train.labels = tweets_train.labels.apply(lambda x:reassign_label(x))

"""# Визуелизација

## Моменталната состојба по држава
"""

c = 0
for i in range(10):
  for j in range(20):
    axs[i, j] = all_countries[c][1].plot.line()
    axs[i, j].set_title(all_countries[c][0])
    c = c + 1 
    if c == 194:
      break

"""## Моменталната состојба во светот

### За вкупно случаи
"""

totalDf['today_confirmed'].plot.line()

"""### За вкупно смртни случаи"""

totalDf['today_deaths'].plot.line()

"""### За вкупно оздравени """

totalDf['today_recovered'].plot.line()

"""### За нови случаи денес"""

totalDf['today_new_confirmed'].plot.line()

"""### За нови смртни случаи денес"""

totalDf['today_new_deaths'].plot.line()

"""### За нови оздравени случаи денес

"""

totalDf['today_new_recovered'].plot.line()

"""## Прикажување на најчестите зборови во множеството што ќе го користиме за тренирање на моделот"""

all_words = []
hashtags_in_text = []

for i in tweets_train.text_without_stopwords[:500]:
    for j in i.split(' '):
      if '#' not in j and 'https' not in j:
        all_words.append(j)
      elif '#' in j:
        hashtags_in_text.append(j)
        
all_words_dict_train = Counter(all_words)
hashtags_in_text_dict_train = Counter(hashtags_in_text)

"""### Приказ од трвитовите"""

def text_size(total):
    return 30 + ((total / 200) * 40)

most_common = []
str_common = ''
stopwords = set(STOPWORDS)

for word, value in all_words_dict_train.most_common(40):
    most_common.append(word)

str_common += ' '.join(most_common)

plt.figure(figsize = (15, 8), facecolor = None)


wordcloud = WordCloud(width = 800, height = 800, 
                    background_color ='white',
                    stopwords = stopwords,
                    min_font_size = 10, mode="RGBA").generate(str_common)
plt.imshow(wordcloud)
plt.axis("off") 
plt.tight_layout(pad = 0)
plt.show()

"""### Приказ од hastagsот"""

def text_size(total):
    return 30 + ((total / 200) * 40)

most_common = []
str_common = ''
stopwords = set(STOPWORDS)

for word, value in hashtags_in_text_dict_train.most_common(40):
    most_common.append(word)

str_common += ' '.join(most_common)

plt.figure(figsize = (15, 8), facecolor = None)

wordcloud = WordCloud(width = 800, height = 800, 
                    background_color ='white',
                    stopwords = stopwords,
                    min_font_size = 10, mode="RGBA").generate(str_common)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off") 
plt.tight_layout(pad = 0)
plt.show()

"""##Прикажување на најчестите зборови во множеството што ќе го користиме за предвидување

"""

all_words = []
hashtags_in_text = []

for i in tweets_test.text_without_stopwords[:500]:
    for j in i.split(' '):
      if '#' not in j and 'https' not in j:
        all_words.append(j)
      elif '#' in j:
        hashtags_in_text.append(j)
        
all_words_dict_test = Counter(all_words)
hashtags_in_text_dict_test = Counter(hashtags_in_text)

"""### Прикажување на твитовите"""

def text_size(total):
    return 30 + ((total / 200) * 40)

most_common = []
str_common = ''
stopwords = set(STOPWORDS)

for word, value in all_words_dict_test.most_common(40):
    most_common.append(word)

str_common += ' '.join(most_common)

plt.figure(figsize = (15, 8), facecolor = None)


wordcloud = WordCloud(width = 800, height = 800, 
                    background_color ='white',
                    stopwords = stopwords,
                    min_font_size = 10, mode="RGBA").generate(str_common)
plt.imshow(wordcloud)
plt.axis("off") 
plt.tight_layout(pad = 0)
plt.show()

"""### Прикажување на hastagот"""

def text_size(total):
    return 30 + ((total / 200) * 40)

most_common = []
str_common = ''
stopwords = set(STOPWORDS)

for word, value in hashtags_in_text_dict_test.most_common(40):
    most_common.append(word)

str_common += ' '.join(most_common)

plt.figure(figsize = (15, 8), facecolor = None)

wordcloud = WordCloud(width = 800, height = 800, 
                    background_color ='white',
                    stopwords = stopwords,
                    min_font_size = 10, mode="RGBA").generate(str_common)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off") 
plt.tight_layout(pad = 0)
plt.show()

"""## Прикажување на labels"""

sns.countplot(x=tweets_train.labels)
plt.title('бројот на елементи во секоја класа', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 15})
plt.tight_layout()
plt.show()

plt.figure(figsize=(15,10))
plt.pie(tweets_train.labels.value_counts(),explode=[0.01,0.01,0.001],colors=['green','red','blue'],
        labels=['Positive','Negative','Neutral'],autopct='%0.2f%%',radius=1,startangle=45)
plt.title('Sentiments',fontdict={'size':'20'})
plt.show()

y = pd.get_dummies(tweets_train.labels)
y

"""# Модел

## Поделба на множестваата
"""

train = tweets_train.text_without_stopwords
test = tweets_test.text_without_stopwords

"""## Енкодирање на множествата

#### Tokenizer
"""

max_features = 1000
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(train)
train = tokenizer.texts_to_sequences(train)
test = tokenizer.texts_to_sequences(test)

"""#### Приказ на графикон"""

totalNumWords = [len(one_comment) for one_comment in train]
plt.hist(totalNumWords,bins = 30)
plt.show()

totalNumWords = [len(one_comment) for one_comment in test]
plt.hist(totalNumWords,bins = 30)
plt.show()

"""#### Pad sequence"""

max_words = 150
train = sequence.pad_sequences(train, maxlen=max_words)
test = sequence.pad_sequences(test, maxlen=max_words)

"""## Градење на моделот"""

K.clear_session()
model = Sequential()
model.add(Embedding(max_features, 150, input_length=150))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(64, activation = 'relu'))
model.add(Dense(3, activation = 'softmax'))

model.layers

model.summary()

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

"""## Тренирање на моделот"""

history = model.fit(train, y, epochs=10,batch_size = 128, verbose=2, validation_split=0.2)

"""## Визуелизација на резултатите"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy'] 
loss = history.history['loss'] 
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'r', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc') 
plt.title('Пресметување на прецизностна на тренинг и валидациското множество') 
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss') 
plt.title('Пресметување на loss фукнцијата')
plt.legend()

plt.show()

"""## Евалуација на моделот"""

model.evaluate(train, y)

"""
# Предвидување"""

y_predict = model.predict(test)
y_predict

"""##Пронаоѓање на вистинската класа"""

y_classes = []
for sample in y_predict:
  if sample[0] >= sample[1]  and sample[0] >= sample[2]:
    y_classes.append(1)
  elif sample[1] >= sample[0]  and sample[1] >= sample[2]:
    y_classes.append(0) 
  elif sample[2] >= sample[0]  and sample[2] >= sample[1]:
    y_classes.append(-1)

y_classes

"""## Визуализација на предвидените резултати"""

sns.countplot(x=y_classes)
plt.title('Предвидени резултати', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 15})
plt.tight_layout()
plt.show()

"""Според предвидувањата може да забележиме дека сликата за состојбата сеуште е неутрална."""